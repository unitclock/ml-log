{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import platform\n",
    "import shutil\n",
    "import pynvml\n",
    "import multiprocessing\n",
    "import yaml\n",
    "from functools import wraps\n",
    "import csv\n",
    "import statistics\n",
    "import sys\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single(cls):\n",
    "    _instance = {}\n",
    "    @wraps(cls)\n",
    "    def _single(*args,**kwargs):\n",
    "        # nonlocal _instance\n",
    "        if cls not in _instance :\n",
    "            _instance[cls] = cls(*args,**kwargs) \n",
    "        else:\n",
    "            pass\n",
    "        return _instance[cls]\n",
    "    return _single\n",
    "\n",
    "def run_as_daemon(func):\n",
    "    @wraps(func)\n",
    "    def _process(*arg,**kwargs):\n",
    "        p    = multiprocessing.Process(target=func,args=arg,kwargs=kwargs,daemon=True)\n",
    "        p.start()\n",
    "        return p\n",
    "    return _process\n",
    "\n",
    "def get_os_info() ->dict:\n",
    "\n",
    "    device = get_gpu_list()\n",
    "    try:\n",
    "        info = {\n",
    "                \"hostname\":platform.node(),\n",
    "                \"platform\":platform.platform(),\n",
    "                \"system\":platform.system(),\n",
    "                \"python_version\":platform.python_version(),\n",
    "                \"architecture\":platform.architecture()[0],\n",
    "                \"processor\":platform.processor(),\n",
    "                \"uname\":str(platform.uname()),\n",
    "                \"cpu_logical_count\":psutil.cpu_count(),\n",
    "                \"cpu_count\": psutil.cpu_count(logical=False),\n",
    "                \"total_memory\": psutil.virtual_memory().total /100000,\n",
    "                \"active_memory\": psutil.virtual_memory().active /100000,\n",
    "                \"available_memory\": psutil.virtual_memory().available /100000,\n",
    "                \"total_swap_memory\":psutil.swap_memory().total /100000,\n",
    "                \"nvidia_gpu_info\":str(device),\n",
    "                \"python_path\":sys.executable,\n",
    "                \"run_path\":os.getcwd()\n",
    "        }\n",
    "    except:\n",
    "        raise BaseException(\"系统信息采集失败\")\n",
    "\n",
    "    return info\n",
    "\n",
    "@run_as_daemon\n",
    "def watch_cpu(path:str):\n",
    "    os.makedirs(path,mode=0o777,exist_ok=True)\n",
    "    sleep_time = 5\n",
    "    cut_time =1800\n",
    "    i =0\n",
    "    count = 0\n",
    "    while True:\n",
    "        with open(f\"{path}/cpu-{count}.log\",\"a\") as f:\n",
    "            while True:\n",
    "                cpu_percent = psutil.cpu_percent()\n",
    "                memory = psutil.virtual_memory().used\n",
    "                f.write(str({\"time\":time.strftime('%Y-%m-%d %X', time.localtime()),\"cpu_percent\":cpu_percent,\"memory\":memory})+\"\\n\")\n",
    "                f.flush()\n",
    "                time.sleep(sleep_time)\n",
    "                i+=1\n",
    "                if i ==(cut_time/sleep_time):\n",
    "                    i =0\n",
    "                    break\n",
    "        count +=1\n",
    "        continue\n",
    "    \n",
    "def save_dict_to_json(dict_value:dict , save_path:str) ->None:\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(json.dumps(dict_value, indent=2))\n",
    "        file.flush()\n",
    "    return\n",
    "\n",
    "def save_dict_to_yaml(dict_value: dict, save_path: str):\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(yaml.dump(dict_value, allow_unicode=True))\n",
    "        file.flush()\n",
    "    return\n",
    "\n",
    "def read_yaml_to_dict(yaml_path: str):\n",
    "    with open(yaml_path) as file:\n",
    "        dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "        return dict_value\n",
    "    \n",
    "def save_list_to_csv(data_list:list, output_file:str)->None:\n",
    "    headers = set()\n",
    "    for item in data_list:\n",
    "        headers.update(item.keys())\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        \n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        for item in data_list:\n",
    "            row = [item.get(key, '') for key in headers]\n",
    "            writer.writerow(row)\n",
    "    return\n",
    "\n",
    "@run_as_daemon\n",
    "def watch_gpu(path:str)->None:\n",
    "        os.makedirs(path,mode=0o777,exist_ok=True)\n",
    "        pynvml.nvmlInit()\n",
    "        sleep_time =5\n",
    "        device_count = pynvml.nvmlDeviceGetCount()\n",
    "        i =0\n",
    "        count = 0\n",
    "        while True:\n",
    "            with open(f\"{path}/gpu-{count}.log\",\"a\") as f:\n",
    "                while True:\n",
    "                    device_status =[]\n",
    "\n",
    "                    for i in range(device_count):\n",
    "                        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                        gpu_percent = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                        gpu_memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                        status = {\"time\":time.strftime('%Y-%m-%d %X', time.localtime()),\"gpu_percent\":gpu_percent.gpu,\"gpu_memory\":gpu_memory.used}\n",
    "                        device_status.append(status)\n",
    "                    f.write(str(device_status)+\"\\n\")\n",
    "                    f.flush()\n",
    "                    time.sleep(sleep_time)\n",
    "                    i+=1\n",
    "                    if i == 1800/sleep_time:\n",
    "                        i = 0\n",
    "                        break\n",
    "                count+=1\n",
    "                continue\n",
    "\n",
    "def is_process_running(main_pid:int) ->bool:\n",
    "    try:\n",
    "        ps = psutil.Process(pid=main_pid)\n",
    "        return ps.is_running\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def save_conda_info(path:str) ->bool:\n",
    "    try:\n",
    "        result = subprocess.run(['conda', 'list'], capture_output=True, text=True)\n",
    "        output = result.stdout\n",
    "        with open(f\"{path}/conda.info\",\"a\") as file:\n",
    "                file.write(output)\n",
    "                file.flush()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_gpu_list() ->list:\n",
    "    device_list =[]\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        device_count=pynvml.nvmlDeviceGetCount()\n",
    "\n",
    "        for i in range(device_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            device_list.append(str(pynvml.nvmlDeviceGetName(handle)))\n",
    "    except:\n",
    "        print(\"未获取到Nvidia显卡信息 \\n\")\n",
    "    return device_list\n",
    "\n",
    "def has_multiple_keys(dictionary:dict, *keys):\n",
    "    return set(keys).issubset(dictionary.keys())\n",
    "\n",
    "\n",
    "\n",
    "def get_init_trainning_status()->dict:\n",
    "    return {\"epoch\":[],\"next_start_at\":0,\"count\":0}\n",
    "\n",
    "def get_process_pid() -> int:\n",
    "    return os.getpid()\n",
    "\n",
    "def get_all_recorded_element(data)->list:\n",
    "    elemet_list = []\n",
    "    for d in data :\n",
    "        elemet_list.extend(list(d.keys()))\n",
    "    result = list(set(elemet_list))\n",
    "    return result\n",
    "    \n",
    "def quick_analysis(status:list) ->dict:\n",
    "    result = {}\n",
    "    element_list = get_all_recorded_element(status)\n",
    "    for e in element_list:\n",
    "        result[e] = []\n",
    "        for s in status:\n",
    "            if e in s.keys():\n",
    "                result[e].append(s[e])\n",
    "        result[e+\"_max\"] = max(result[e])\n",
    "        result[e+\"_min\"] = min(result[e])\n",
    "        result[e+\"_viriance\"]=statistics.variance(result[e])\n",
    "        result[e+\"_stdev\"]=statistics.stdev(result[e])\n",
    "        result[e+\"_avg\"] = statistics.mean(result[e])\n",
    "    return result\n",
    "\n",
    "def format_time() ->str:\n",
    "    return time.strftime(\"%Y-%m-%d %X\", time.localtime())\n",
    "                \n",
    "def copy_file_to_dir(srcfile,dstpath):                       # 复制函数\n",
    "    if not os.path.isfile(srcfile):\n",
    "        print (\"%s not exist!\"%(srcfile))\n",
    "    else:\n",
    "        fpath,fname=os.path.split(srcfile)             # 分离文件名和路径\n",
    "        if not os.path.exists(dstpath):\n",
    "            os.makedirs(dstpath)                       # 创建路径\n",
    "        shutil.copy(srcfile, dstpath + fname)          # 复制文件\n",
    "\n",
    "def get_experiment_id() -> str:\n",
    "\n",
    "    digits = [str(random.randint(0, 9)) for _ in range(19)]\n",
    "\n",
    "    result = \"\".join(digits)\n",
    "    return  result \n",
    "    \n",
    "def api(url:str,data:dict) ->dict:\n",
    "    header = {'Content-Type': 'application/json'}\n",
    "    resp = requests.post(url=url,headers=header,json=data)\n",
    "    print(f\"{format_time} {data} {url}\")\n",
    "    msg =dict(resp.json())\n",
    "    print(f\"{format_time} {msg} \")\n",
    "    return msg\n",
    "    \n",
    "def append_fo_file(path:str,sth:str):\n",
    "    with open(path,\"a\") as file:\n",
    "            file.write(sth)\n",
    "            file.flush()\n",
    "    return    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewLogger(conf:dict,info:dict):\n",
    "    log = Logger(config=conf)\n",
    "    log.Start(info=info)\n",
    "    return log\n",
    "\n",
    "\n",
    "class Printer():\n",
    "        def __init__(self,run_path:str) ->None:\n",
    "            if os.path.exists(run_path):\n",
    "                self.__location = run_path\n",
    "                append_fo_file(self.__location,f\"console init {format_time()}\")\n",
    "            return\n",
    "        \n",
    "        def Print(self,sth:str):\n",
    "            append_fo_file(self.__location,sth=sth)\n",
    "            return\n",
    "        \n",
    "@single\n",
    "class Logger():\n",
    "    def __init__(self,config:dict) -> None:\n",
    "\n",
    "        if not has_multiple_keys(config, 'access_token', 'project',\"description\",\"experiment_name\"):\n",
    "            raise BaseException(\"缺失启动信息,请补充config参数: access_token project description experiment ;可选配置项: repository_id\")\n",
    "        config[\"experiment_id\"] = get_experiment_id()\n",
    "        self.__config = config\n",
    "        self.__verify_my_client()\n",
    "        self.__save_config()\n",
    "\n",
    "        self.__trainning_status  = get_init_trainning_status()\n",
    "        return\n",
    "\n",
    "    def Print(self,sth:str):\n",
    "        save_path =f\"{self.__location}/{self.__runid}/console.log\"\n",
    "        append_fo_file(path=save_path,sth=sth)            \n",
    "        pass\n",
    "\n",
    "        \n",
    "    def __verify_my_client(self,host=\"127.0.0.1\") ->None:\n",
    "\n",
    "        self.__api_load_save_path = f\"http://{host}:5560/ml_client/client/loadSavePath\"\n",
    "        self.__api_notice_experiment = f\"http://{host}:5560/ml_client/client/noticeExperiment\"\n",
    "        self.__api_notice_run = f\"http://{host}:5560/ml_client/client/noticeRun\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            send_data={}\n",
    "            send_data[\"userToken\"] = self.__config[\"access_token\"]\n",
    "            send_data[\"projectId\"] = self.__config[\"project\"]\n",
    "            send_data[\"description\"] = self.__config[\"description\"]\n",
    "            send_data[\"experimentName\"] = self.__config[\"experiment_name\"]\n",
    "            try:\n",
    "                send_data[\"repositoryId\"] = self.__config[\"repository_id\"]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            msg =api(url=self.__api_load_save_path,data=send_data)\n",
    "            if not msg[\"code\"] == 200:\n",
    "                raise ConnectionError\n",
    "            self.__location = msg[\"data\"]\n",
    "        except:\n",
    "            raise ConnectionError\n",
    "        else:\n",
    "            self.__location = f\"{self.__location}/{self.__config['experiment_id']}\"\n",
    "            self.__savedir = f\"{self.__location}/code\"\n",
    "            self.__codedir = os.getcwd()\n",
    "            i = 0\n",
    "            self.__runid = f\"run-{i}\"\n",
    "\n",
    "        os.makedirs(f\"{self.__location}\",mode=0o777,exist_ok=True)\n",
    "        return\n",
    "\n",
    "    def __watcher(self) ->None:\n",
    "        cpu_dir =f\"{self.__location}/{self.__runid}/watcher/cpu\"\n",
    "        self.__watcher_cpu = watch_cpu(path=cpu_dir)\n",
    "        gpu_dir =f\"{self.__location}/{self.__runid}/watcher/gpu\"\n",
    "        self.__watcher_gpu = watch_gpu(path=gpu_dir)\n",
    "        return\n",
    "\n",
    "    def Start(self,info:dict) ->None:\n",
    "\n",
    "        if not save_conda_info(self.__location):\n",
    "            print(\"未采集到conda信息\")\n",
    "        try:\n",
    "            self.__osinfo = get_os_info()\n",
    "            \n",
    "            os_info_json_path =f\"{self.__location}/os_info.json\"\n",
    "            os_info_yaml_path =f\"{self.__location}/os_info.yaml\"\n",
    "\n",
    "            save_dict_to_json(self.__osinfo,os_info_json_path)\n",
    "            save_dict_to_yaml(self.__osinfo,os_info_yaml_path)\n",
    "            \n",
    "            super_arg_json_path = self.__location+\"/super_arg.json\"\n",
    "            super_arg_yaml_path = self.__location+\"/super_arg.yaml\"\n",
    "\n",
    "            save_dict_to_json(info,super_arg_json_path)\n",
    "            save_dict_to_yaml(info,super_arg_yaml_path)\n",
    "            \n",
    "            with open(f\"{self.__location}/start.tag\",mode=\"w\") as f:\n",
    "                f.write(f\"{format_time()} | {self.__runid}\\n\")\n",
    "                f.flush()\n",
    "            if os.system(\"pip freeze > requirements.txt\") == 0:\n",
    "                if os.path.exists(\"./requirements.txt\"):\n",
    "                    shutil.copy(\"./requirements.txt\",f\"{self.__location}\")    \n",
    "            send_data = {\n",
    "                        \"experimentId\":self.__config[\"experiment_id\"],\n",
    "                        \"status\":0\n",
    "                        }\n",
    "\n",
    "            resp = api(url=self.__api_notice_experiment,data=send_data)\n",
    "            if not resp[\"code\"] == 200:\n",
    "                raise ConnectionError      \n",
    "        except:\n",
    "            raise BaseException(\"日志实例启动失败\\n\")\n",
    "        return\n",
    "    \n",
    "    def Save(self,path_list:list):\n",
    "        for path in path_list:\n",
    "            if os.path.exists(path):\n",
    "                file_name = os.path.basename(path)\n",
    "                copy_file_to_dir(file_name,f\"{self.__location}/{self.__runid}/files/\")\n",
    "                with open(f\"{self.__location}/{self.__runid}/file.tag\",\"a\") as f:\n",
    "                    f.write(f\"files/{file_name}\\n\")\n",
    "                    f.flush()\n",
    "        return\n",
    "        \n",
    "    def Run(self) ->None:\n",
    "\n",
    "        os.makedirs(f\"{self.__location}/{self.__runid}\",exist_ok=True)\n",
    "\n",
    "        with open(f\"{self.__location}/{self.__runid}/start.tag\",mode=\"w\") as f:\n",
    "            f.write(f\"{format_time()} | {self.__runid}\\n\")\n",
    "            f.flush()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        #通知客户端开始\n",
    "        send_data ={\n",
    "                        \"experimentId\":self.__config[\"experiment_id\"],\n",
    "                        \"runName\": self.__runid,\n",
    "                        \"status\": 0\n",
    "                    }\n",
    "        \n",
    "        resp = api(url=self.__api_notice_run,data=send_data)\n",
    "        if not resp[\"code\"] == 200:\n",
    "            raise ConnectionError\n",
    "\n",
    "        self.__watcher()\n",
    "            \n",
    "        return\n",
    "\n",
    "    def Log(self,info:dict) ->None:\n",
    "        try:\n",
    "            self.__trainning_status[\"count\"] += 1\n",
    "            self.__trainning_status[\"epoch\"].append(info)\n",
    "        except:\n",
    "            raise BaseException(\"Epoch日志采集失败\")\n",
    "        return \n",
    "    \n",
    "    def End(self) ->None:\n",
    "\n",
    "        \n",
    "        send_data ={\n",
    "                        \"experimentId\":self.__config[\"experiment_id\"],\n",
    "                        \"runName\": self.__runid,\n",
    "                        \"status\": 1\n",
    "                    }\n",
    "        resp = api(url=self.__api_notice_run,data=send_data)\n",
    "        if not resp[\"code\"] == 200:\n",
    "            raise ConnectionError\n",
    "        result_path =f\"{self.__location}/{self.__runid}/results.json\"\n",
    "        save_dict_to_json(self.__trainning_status[\"epoch\"][self.__trainning_status[\"next_start_at\"]:self.__trainning_status[\"count\"]],result_path)\n",
    "        result_csv_path=f\"{self.__location}/{self.__runid}/results.csv\"\n",
    "\n",
    "        dict = quick_analysis(status=self.__trainning_status[\"epoch\"][self.__trainning_status[\"next_start_at\"]:self.__trainning_status[\"count\"]])\n",
    "        analysis_json_path = f\"{self.__location}/{self.__runid}/analysis.json\"\n",
    "        analysis_yaml_path = f\"{self.__location}/{self.__runid}/analysis.yaml\"\n",
    "        save_dict_to_json(dict,analysis_json_path)\n",
    "        save_dict_to_yaml(dict,analysis_yaml_path)\n",
    "\n",
    "        save_list_to_csv(self.__trainning_status[\"epoch\"][self.__trainning_status[\"next_start_at\"]:self.__trainning_status[\"count\"]],result_csv_path)\n",
    "        self.__trainning_status[\"next_start_at\"] = self.__trainning_status[\"count\"]\n",
    "        last = self.__trainning_status[\"epoch\"][len(self.__trainning_status[\"epoch\"])-1]\n",
    "        last_result_path = f\"{self.__location}/{self.__runid}/last.json\"\n",
    "        save_dict_to_json(last,last_result_path)\n",
    "\n",
    "\n",
    "        self.__kill_watcher()\n",
    "\n",
    "        i=0\n",
    "        while os.path.exists(f\"{self.__location}/run-{i}\"):\n",
    "            i+=1   \n",
    "        with open(f\"{self.__location}/{self.__runid}/finish.tag\",mode=\"a\") as f:\n",
    "            f.write(f\"{format_time()} | {self.__runid} \\n\")\n",
    "            f.flush()     \n",
    "        self.__runid = f\"run-{i}\"\n",
    "        return\n",
    "    \n",
    "    def __kill_watcher(self):\n",
    "        self.__watcher_cpu.kill()\n",
    "        self.__watcher_gpu.kill()\n",
    "        return\n",
    "    \n",
    "    def Submit(self) ->None:\n",
    "        try:\n",
    "            send_data = {\n",
    "                            \"experimentId\":self.__config[\"experiment_id\"],\n",
    "                            \"status\":1\n",
    "                        }\n",
    "\n",
    "            resp = api(url=self.__api_notice_experiment,data=send_data)\n",
    "            if not resp[\"code\"] == 200:\n",
    "                raise ConnectionError\n",
    "            self.__save_code()\n",
    "\n",
    "            # result_csv_path = f\"{self.__location}/result.csv\"\n",
    "            # save_list_to_csv(self.__trainning_status[\"epoch\"],result_csv_path)\n",
    "\n",
    "            dict = quick_analysis(status=self.__trainning_status[\"epoch\"])\n",
    "            analysis_json_path = f\"{self.__location}/analysis.json\"\n",
    "            analysis_yaml_path = f\"{self.__location}/analysis.yaml\"\n",
    "            save_dict_to_json(dict,analysis_json_path)\n",
    "            save_dict_to_yaml(dict,analysis_yaml_path)\n",
    "            \n",
    "            with open(f\"{self.__location}/finish.tag\",mode=\"a\") as f:\n",
    "                f.write(f\"{format_time()} | finish \\n\")\n",
    "                f.flush() \n",
    "\n",
    "        except:\n",
    "            raise BaseException(\"END ERROR\")\n",
    "        return\n",
    "    \n",
    "    def __save_config(self) ->None:\n",
    "        try:\n",
    "            config_path_json = self.__location +\"/\"+\"config.json\"\n",
    "            save_dict_to_json(self.__config,config_path_json)\n",
    "            config_path_yaml = self.__location +\"/\"+\"config.yaml\"\n",
    "            save_dict_to_yaml(self.__config,config_path_yaml)\n",
    "        except:\n",
    "            raise BaseException(\"保存配置信息失败 \\n\")\n",
    "        return\n",
    "\n",
    "    def __save_code(self,path=[\"datasets\"])->None:\n",
    "        ignore_path = [*path]\n",
    "        try:\n",
    "            if os.path.exists(\".path_ignore\"):\n",
    "                with open(\".path_ignore\") as f:\n",
    "                    line =  f.readline()\n",
    "                    while line:\n",
    "                        ignore_path.append(line.strip())\n",
    "                        line = f.readline()\n",
    "            shutil.copytree(src=self.__codedir,dst=self.__savedir,dirs_exist_ok=True,ignore=shutil.ignore_patterns(*ignore_path))\n",
    "        except:\n",
    "            raise BaseException(\"备份代码失败 \\n\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function format_time at 0x7fa778d55f30> {'userToken': 'eyJhbGciOiJIUzI1NiIsInppcCI6IkdaSVAifQ.H4sIAAAAAAAAAKtWKi5NUrJS8kotcSwtyff1UdJRSq0oULIyNLO0MLU0NTIw0FEqLU4t8kwBi1maGxhaWBoaGJgZGhqam5hBJP0Sc1OBhuRkVibmpZfk56Ur1QIAeD1whlkAAAA.EuSe_vlTjAuPaPsc8AHLvNJ0yhqQX3WHVFnap9eWvbI', 'projectId': '570', 'description': 'description', 'experimentName': 'experiment_name', 'repositoryId': 'e4107e9add2646d9b85a6a4c9fa43136'} http://127.0.0.1:5560/ml_client/client/loadSavePath\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '/opt/jml_ai_result'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'status': 0} http://127.0.0.1:5560/ml_client/client/noticeExperiment\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'runName': 'run-0', 'status': 0} http://127.0.0.1:5560/ml_client/client/noticeRun\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'runName': 'run-0', 'status': 1} http://127.0.0.1:5560/ml_client/client/noticeRun\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'runName': 'run-1', 'status': 0} http://127.0.0.1:5560/ml_client/client/noticeRun\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'runName': 'run-1', 'status': 1} http://127.0.0.1:5560/ml_client/client/noticeRun\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'runName': 'run-2', 'status': 0} http://127.0.0.1:5560/ml_client/client/noticeRun\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'runName': 'run-2', 'status': 1} http://127.0.0.1:5560/ml_client/client/noticeRun\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n",
      "<function format_time at 0x7fa778d55f30> {'experimentId': '6962309413508863015', 'status': 1} http://127.0.0.1:5560/ml_client/client/noticeExperiment\n",
      "<function format_time at 0x7fa778d55f30> {'msg': '操作成功', 'code': 200, 'data': '操作成功'} \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN,self).__init__()\n",
    "            '''\n",
    "            一般来说，卷积网络包括以下内容：\n",
    "            1.卷积层\n",
    "            2.神经网络\n",
    "            3.池化层\n",
    "            '''\n",
    "            self.conv1=nn.Sequential(\n",
    "                nn.Conv2d(              #--> (1,28,28)\n",
    "                    in_channels=1,      #传入的图片灰度图\n",
    "                    out_channels=16,    #输出的图片\n",
    "                    kernel_size=5,      #卷积核为5*5\n",
    "                    stride=1,           #跳步步长\n",
    "                    padding=2,          #边框补全，其计算公式=（kernel_size-1）/2=(5-1)/2=2\n",
    "                ),    # 2d代表二维卷积           --> (16,28,28)\n",
    "                nn.ReLU(),              #非线性激活层\n",
    "                nn.MaxPool2d(kernel_size=2),    #设定这里的扫描区域为2*2，且取出该2*2中的最大值          --> (16,14,14)\n",
    "            )\n",
    "\n",
    "            self.conv2=nn.Sequential(\n",
    "                nn.Conv2d(              #       --> (16,14,14)\n",
    "                    in_channels=16,     #这里的输入是上层的输出为16层\n",
    "                    out_channels=32,    #在这里我们需要将其输出为32层\n",
    "                    kernel_size=5,      #代表扫描的区域点为5*5\n",
    "                    stride=1,           #就是每隔多少步跳一下\n",
    "                    padding=2,          #边框补全，其计算公式=（kernel_size-1）/2=(5-1)/2=\n",
    "                ),                      #   --> (32,14,14)\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),    #设定这里的扫描区域为2*2，且取出该2*2中的最大值     --> (32,7,7)，这里是三维数据\n",
    "            )\n",
    "\n",
    "            self.out=nn.Linear(32*7*7,10)       #注意一下这里的数据是二维的数据\n",
    "\n",
    "        def forward(self,x):\n",
    "            x=self.conv1(x)\n",
    "            x=self.conv2(x)     #（batch,32,7,7）\n",
    "            #然后接下来进行一下扩展展平的操作，将三维数据转为二维的数据\n",
    "            x=x.view(x.size(0),-1)    #(batch ,32 * 7 * 7)\n",
    "            output=self.out(x)\n",
    "            \n",
    "            return output\n",
    "\n",
    "#Hyper prameters\n",
    "EPOCH=3\n",
    "BATCH_SIZE=64\n",
    "LR=0.001\n",
    "DOWNLOAD_MNIST=False\n",
    "\n",
    "log = NewLogger(\n",
    "\n",
    "    conf={\n",
    "    'access_token':\"eyJhbGciOiJIUzI1NiIsInppcCI6IkdaSVAifQ.H4sIAAAAAAAAAKtWKi5NUrJS8kotcSwtyff1UdJRSq0oULIyNLO0MLU0NTIw0FEqLU4t8kwBi1maGxhaWBoaGJgZGhqam5hBJP0Sc1OBhuRkVibmpZfk56Ur1QIAeD1whlkAAAA.EuSe_vlTjAuPaPsc8AHLvNJ0yhqQX3WHVFnap9eWvbI\",\n",
    "    'project':\"570\", \n",
    "    \"description\":\"description\",\n",
    "    \"experiment_name\":\"experiment_name\",\n",
    "    \"repository_id\":\"e4107e9add2646d9b85a6a4c9fa43136\"\n",
    "    },\n",
    "\n",
    "    info={\n",
    "    \"learnning_rate\":LR,\n",
    "    \"epoch\":EPOCH,\n",
    "    \"batch_size\":BATCH_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "run_count = 0\n",
    "while run_count<3:\n",
    "\n",
    "    log.Run()\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\") \n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "    train_data=torchvision.datasets.MNIST(\n",
    "        root='./mnist',\n",
    "        train=True,\n",
    "        transform=torchvision.transforms.ToTensor(),    #将下载的文件转换成pytorch认识的tensor类型，且将图片的数值大小从（0-255）归一化到（0-1）\n",
    "        download=DOWNLOAD_MNIST\n",
    "    )\n",
    "\n",
    "    # print(train_data.data.size())\n",
    "    # print(train_data.targets.size())\n",
    "    # plt.imshow(train_data.data[0].numpy(),cmap='gray')\n",
    "    # plt.title('%i'%train_data.targets[0])\n",
    "    # plt.show()\n",
    "\n",
    "    train_loader=Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_data=torchvision.datasets.MNIST(\n",
    "        root='./mnist',\n",
    "        train=False,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        test_x=Variable(torch.unsqueeze(test_data.data, dim=1)).type(torch.cuda.FloatTensor)[:2000]/255  \n",
    "        test_y=test_data.targets[:2000]\n",
    "        test_y.cuda()\n",
    "\n",
    "\n",
    "            \n",
    "    cnn=CNN()\n",
    "    # print(cnn)\n",
    "    cnn.to(device=device)\n",
    "\n",
    "    # 添加优化方法\n",
    "    optimizer=torch.optim.Adam(cnn.parameters(),lr=LR)\n",
    "    # 指定损失函数使用交叉信息熵\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "    step=0\n",
    "    for e in range(EPOCH):\n",
    "        #加载训练数据\n",
    "        for step,(x,y) in enumerate(train_loader):\n",
    "            #分别得到训练数据的x和y的取值\n",
    "            b_x=Variable(x.to(device))\n",
    "            b_y=Variable(y.to(device))\n",
    "            output=cnn(b_x)         #调用模型预测\n",
    "            loss=loss_fn(output,b_y)#计算损失值\n",
    "            optimizer.zero_grad()   #每一次循环之前，将梯度清零\n",
    "            loss.backward()         #反向传播\n",
    "            optimizer.step()        #梯度下降\n",
    "\n",
    "            count = 1\n",
    "            #每执行count次，输出一下当前epoch、loss、accuracy\n",
    "            if (step%count==0):\n",
    "                #计算一下模型预测正确率\n",
    "                test_output=cnn(test_x)\n",
    "                y_pred=torch.max(test_output,1)[1].data.squeeze()\n",
    "                accuracy=sum(y_pred==test_y.cuda()).item()/test_y.cuda().size(0)\n",
    "               \n",
    "                log.Log({\"epoch\":e,\"loss\":loss.item(),\"accuracy\":accuracy})\n",
    "                log.Save([\"1.png\"])\n",
    "                # print('now epoch :  ', epoch, '   |  loss : %.4f ' % loss.item(), '     |   accuracy :   ' , accuracy)\n",
    "        model_path = \"model.pt\"\n",
    "        torch.save(cnn,model_path)\n",
    "        log.Save([model_path])\n",
    "\n",
    "    test_output=cnn(test_x[:10])\n",
    "    y_pred=torch.max(test_output,1)[1].data.squeeze()       #选取最大可能的数值所在的位置\n",
    "\n",
    "    \n",
    "\n",
    "    log.Print(f\"预测值: {y_pred.tolist()}\")\n",
    "    log.Print(f\"实际值: {test_y[:10].tolist()}\")\n",
    "    \n",
    "    log.End()\n",
    "    run_count +=1\n",
    "\n",
    "log.Submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
