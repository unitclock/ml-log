{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import platform\n",
    "import shutil\n",
    "import pynvml\n",
    "import multiprocessing\n",
    "import yaml\n",
    "from functools import wraps\n",
    "import csv\n",
    "import statistics\n",
    "import sys\n",
    "import uuid\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single(cls):\n",
    "    _instance = {}\n",
    "    @wraps(cls)\n",
    "    def _single(*args,**kwargs):\n",
    "        # nonlocal _instance\n",
    "        if cls not in _instance :\n",
    "            _instance[cls] = cls(*args,**kwargs) \n",
    "        else:\n",
    "            pass\n",
    "        return _instance[cls]\n",
    "    return _single\n",
    "\n",
    "def run_as_daemon(func):\n",
    "    @wraps(func)\n",
    "    def _process(*arg,**kwargs):\n",
    "        p    = multiprocessing.Process(target=func,args=arg,kwargs=kwargs,daemon=True)\n",
    "        p.start()\n",
    "        return p\n",
    "    return _process\n",
    "\n",
    "def get_os_info() ->dict:\n",
    "\n",
    "    device = get_gpu_list()\n",
    "    try:\n",
    "        print(\"采集系统信息\")\n",
    "        info = {\n",
    "                \"hostname\":platform.node(),\n",
    "                \"platform\":platform.platform(),\n",
    "                \"system\":platform.system(),\n",
    "                \"python_version\":platform.python_version(),\n",
    "                \"architecture\":platform.architecture()[0],\n",
    "                \"processor\":platform.processor(),\n",
    "                \"uname\":str(platform.uname()),\n",
    "                \"cpu_logical_count\":psutil.cpu_count(),\n",
    "                \"cpu_count\": psutil.cpu_count(logical=False),\n",
    "                \"total_memory\": psutil.virtual_memory().total /100000,\n",
    "                \"active_memory\": psutil.virtual_memory().active /100000,\n",
    "                \"available_memory\": psutil.virtual_memory().available /100000,\n",
    "                \"total_swap_memory\":psutil.swap_memory().total /100000,\n",
    "                \"nvidia_gpu_info\":str(device),\n",
    "                \"python_path\":sys.executable,\n",
    "                \"run_path\":os.getcwd()\n",
    "        }\n",
    "    except:\n",
    "        raise BaseException(\"系统信息采集失败\")\n",
    "\n",
    "    return info\n",
    "\n",
    "@run_as_daemon\n",
    "def watch_cpu(path:str):\n",
    "    os.makedirs(path,mode=0o777,exist_ok=True)\n",
    "    sleep_time = 5\n",
    "    cut_time =1800\n",
    "    i =0\n",
    "    count = 0\n",
    "    while True:\n",
    "        with open(f\"{path}/cpu-{count}.log\",\"a\") as f:\n",
    "            while True:\n",
    "                cpu_percent = psutil.cpu_percent()\n",
    "                memory = psutil.virtual_memory().used\n",
    "                f.write(str({\"time\":time.strftime('%Y-%m-%d %X', time.localtime()),\"cpu_percent\":cpu_percent,\"memory\":memory})+\"\\n\")\n",
    "                f.flush()\n",
    "                time.sleep(sleep_time)\n",
    "                i+=1\n",
    "                if i ==(cut_time/sleep_time):\n",
    "                    i =0\n",
    "                    break\n",
    "        count +=1\n",
    "        continue\n",
    "    \n",
    "def save_dict_to_json(dict_value:dict , save_path:str) ->None:\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(json.dumps(dict_value, indent=2))\n",
    "        file.flush()\n",
    "    return\n",
    "\n",
    "def save_dict_to_yaml(dict_value: dict, save_path: str):\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(yaml.dump(dict_value, allow_unicode=True))\n",
    "        file.flush()\n",
    "    return\n",
    "\n",
    "def read_yaml_to_dict(yaml_path: str):\n",
    "    with open(yaml_path) as file:\n",
    "        dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "        return dict_value\n",
    "    \n",
    "def save_list_to_csv(data_list:list, output_file:str)->None:\n",
    "    headers = set()\n",
    "    for item in data_list:\n",
    "        headers.update(item.keys())\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        \n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        for item in data_list:\n",
    "            row = [item.get(key, '') for key in headers]\n",
    "            writer.writerow(row)\n",
    "    return\n",
    "\n",
    "@run_as_daemon\n",
    "def watch_gpu(path:str)->None:\n",
    "        os.makedirs(path,mode=0o777,exist_ok=True)\n",
    "        pynvml.nvmlInit()\n",
    "        sleep_time =5\n",
    "        device_count = pynvml.nvmlDeviceGetCount()\n",
    "        i =0\n",
    "        count = 0\n",
    "        while True:\n",
    "            with open(f\"{path}/gpu-{count}.log\",\"a\") as f:\n",
    "                while True:\n",
    "                    device_status =[]\n",
    "\n",
    "                    for i in range(device_count):\n",
    "                        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                        gpu_percent = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                        gpu_memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                        status = {\"time\":time.strftime('%Y-%m-%d %X', time.localtime()),\"gpu_percent\":gpu_percent.gpu,\"gpu_memory\":gpu_memory.used}\n",
    "                        device_status.append(status)\n",
    "                    f.write(str(device_status)+\"\\n\")\n",
    "                    f.flush()\n",
    "                    time.sleep(sleep_time)\n",
    "                    i+=1\n",
    "                    if i == 1800/sleep_time:\n",
    "                        i = 0\n",
    "                        break\n",
    "                count+=1\n",
    "                continue\n",
    "\n",
    "def is_process_running(main_pid:int) ->bool:\n",
    "    try:\n",
    "        ps = psutil.Process(pid=main_pid)\n",
    "        return ps.is_running\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def save_conda_info(path:str) ->bool:\n",
    "    try:\n",
    "        result = subprocess.run(['conda', 'list'], capture_output=True, text=True)\n",
    "        output = result.stdout\n",
    "        with open(f\"{path}/conda.info\",\"a\") as file:\n",
    "                file.write(output)\n",
    "                file.flush()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_gpu_list() ->list:\n",
    "    device_list =[]\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        device_count=pynvml.nvmlDeviceGetCount()\n",
    "\n",
    "        for i in range(device_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            device_list.append(str(pynvml.nvmlDeviceGetName(handle)))\n",
    "    except:\n",
    "        print(\"未获取到Nvidia显卡信息 \\n\")\n",
    "    return device_list\n",
    "\n",
    "def has_multiple_keys(dictionary:dict, *keys):\n",
    "    return set(keys).issubset(dictionary.keys())\n",
    "\n",
    "\n",
    "\n",
    "def get_init_trainning_status()->dict:\n",
    "    return {\"epoch\":[],\"next_start_at\":0,\"count\":0}\n",
    "\n",
    "def get_process_pid() -> int:\n",
    "    return os.getpid()\n",
    "\n",
    "def get_all_recorded_element(data)->list:\n",
    "    elemet_list = []\n",
    "    for d in data :\n",
    "        elemet_list.extend(list(d.keys()))\n",
    "    result = list(set(elemet_list))\n",
    "    print(result)\n",
    "    return result\n",
    "    \n",
    "def quick_analysis(status:list) ->dict:\n",
    "    result = {}\n",
    "    element_list = get_all_recorded_element(status)\n",
    "    for e in element_list:\n",
    "        result[e] = []\n",
    "        for s in status:\n",
    "            if e in s.keys():\n",
    "                result[e].append(s[e])\n",
    "        result[e+\"_max\"] = max(result[e])\n",
    "        result[e+\"_min\"] = min(result[e])\n",
    "        result[e+\"_viriance\"]=statistics.variance(result[e])\n",
    "        result[e+\"_stdev\"]=statistics.stdev(result[e])\n",
    "        result[e+\"_avg\"] = statistics.mean(result[e])\n",
    "    return result\n",
    "\n",
    "def format_time() ->str:\n",
    "    return time.strftime(\"%Y-%m-%d %X\", time.localtime())\n",
    "                \n",
    "def copy_file_to_dir(srcfile,dstpath):                       # 复制函数\n",
    "    if not os.path.isfile(srcfile):\n",
    "        print (\"%s not exist!\"%(srcfile))\n",
    "    else:\n",
    "        fpath,fname=os.path.split(srcfile)             # 分离文件名和路径\n",
    "        if not os.path.exists(dstpath):\n",
    "            os.makedirs(dstpath)                       # 创建路径\n",
    "        shutil.copy(srcfile, dstpath + fname)          # 复制文件\n",
    "\n",
    "def get_experiment_id() -> str:\n",
    "\n",
    "    digits = [str(random.randint(0, 9)) for _ in range(20)]\n",
    "\n",
    "    result = \"\".join(digits)\n",
    "    return  result \n",
    "    \n",
    "def api(url:str,data:dict) ->dict:\n",
    "    header = {'Content-Type': 'application/json'}\n",
    "    resp = requests.post(url=url,headers=header,json=data)\n",
    "    return dict(resp.json())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewLogger(conf:dict,info:dict):\n",
    "    log = Logger(config=conf)\n",
    "    log.Start(info=info)\n",
    "    return log\n",
    "        \n",
    "\n",
    "@single\n",
    "class Logger():\n",
    "    def __init__(self,config:dict) -> None:\n",
    "\n",
    "        if not has_multiple_keys(config, 'access_token', 'project',\"description\"):\n",
    "            raise BaseException(\"缺失启动信息,请补充config参数\")\n",
    "        config[\"experiment\"] = get_experiment_id()\n",
    "        self.__config = config\n",
    "        self.__verify_my_client()\n",
    "        self.__save_config()\n",
    "\n",
    "        self.__trainning_status  = get_init_trainning_status()\n",
    "        self.__watcher()\n",
    "\n",
    "        \n",
    "    def __verify_my_client(self,host=\"127.0.0.1\") ->None:\n",
    "\n",
    "        self.__api_load_save_path = f\"http://{host}:5560/ml_client/client/loadSavePath\"\n",
    "        self.__api_notice_experiment = f\"http://{host}:5560/ml_client/client/noticeExperiment\"\n",
    "        self.__api_notice_run = f\"http://{host}:5560/ml_client/client/noticeRun\"\n",
    "\n",
    "        try:\n",
    "            send_data = {}\n",
    "            send_data[\"experimentId\"] = self.__config[\"experiment\"]\n",
    "            send_data[\"status\"] = 0\n",
    "            msg = api(url=self.__api_notice_experiment,data=send_data)\n",
    "            if not msg[\"code\"] == 200:\n",
    "                raise ConnectionError\n",
    "            \n",
    "\n",
    "            send_data={}\n",
    "            send_data[\"userToken\"] = self.__config[\"access_token\"]\n",
    "            send_data[\"projectId\"] = self.__config[\"project\"]\n",
    "            send_data[\"description\"] = self.__config[\"description\"]\n",
    "            msg =api(url=self.__api_load_save_path,data=send_data)\n",
    "            if not msg[\"code\"] == 200:\n",
    "                raise ConnectionError\n",
    "            self.__location = msg[\"data\"]\n",
    "        except:\n",
    "            raise ConnectionError\n",
    "        else:\n",
    "            print(\"代理客户端验证通过 \\n\")\n",
    "            self.__location = f\"{self.__location}/{self.__config['experiment']}\"\n",
    "            self.__savedir = f\"{self.__location}/code\"\n",
    "            self.__codedir = os.getcwd()\n",
    "            i = 0\n",
    "            self.__runid = f\"run-{i}\"\n",
    "            \n",
    "            # while os.path.exists(f\"{self.__location}/run-{i}\"):\n",
    "            #     i+=1\n",
    "            #     self.__runid = f\"run-{i}\"\n",
    "\n",
    "        os.makedirs(f\"{self.__location}\",mode=0o777,exist_ok=True)\n",
    "        return\n",
    "\n",
    "    def __watcher(self) ->None:\n",
    "        cpu_dir =f\"{self.__location}/{self.__runid}/watcher/cpu\"\n",
    "        self.__watcher_cpu = watch_cpu(path=cpu_dir)\n",
    "        gpu_dir =f\"{self.__location}/{self.__runid}/watcher/gpu\"\n",
    "        self.__watcher_gpu = watch_gpu(path=gpu_dir)\n",
    "        return\n",
    "\n",
    "    def Start(self,info:dict) ->None:\n",
    "\n",
    "        if not save_conda_info(self.__location):\n",
    "            print(\"未采集到conda信息\")\n",
    "        try:\n",
    "            self.__osinfo = get_os_info()\n",
    "            \n",
    "            os_info_json_path =f\"{self.__location}/os_info.json\"\n",
    "            os_info_yaml_path =f\"{self.__location}/os_info.yaml\"\n",
    "\n",
    "            save_dict_to_json(self.__osinfo,os_info_json_path)\n",
    "            save_dict_to_yaml(self.__osinfo,os_info_yaml_path)\n",
    "            \n",
    "            super_arg_json_path = self.__location+\"/super_arg.json\"\n",
    "            super_arg_yaml_path = self.__location+\"/super_arg.yaml\"\n",
    "\n",
    "            save_dict_to_json(info,super_arg_json_path)\n",
    "            save_dict_to_yaml(info,super_arg_yaml_path)\n",
    "            \n",
    "            with open(f\"{self.__location}/start.tag\",mode=\"w\") as f:\n",
    "                f.write(f\"{format_time()} | {self.__runid} \\n\")\n",
    "                f.flush()\n",
    "            if os.system(\"pip freeze > requirements.txt\") == 0:\n",
    "                if os.path.exists(\"./requirements.txt\"):\n",
    "                    shutil.copy(\"./requirements.txt\",f\"{self.__location}\")          \n",
    "        except:\n",
    "            raise BaseException(\"日志实例启动失败\\n\")\n",
    "        return\n",
    "    \n",
    "    def Save(self,path_list:list):\n",
    "        for path in path_list:\n",
    "            if os.path.exists(path):\n",
    "                file_name = os.path.basename(path)\n",
    "                copy_file_to_dir(file_name,f\"{self.__location}/{self.__runid}/files/\")\n",
    "                with open(f\"{self.__location}/{self.__runid}/file.tag\",\"a\") as f:\n",
    "                    f.write(f\"{self.__runid}/files/{file_name} \\n\")\n",
    "                    f.flush()\n",
    "        return\n",
    "        \n",
    "    def Run(self) ->None:        \n",
    "        os.makedirs(f\"{self.__location}/{self.__runid}\",exist_ok=True)\n",
    "\n",
    "        with open(f\"{self.__location}/{self.__runid}/start.tag\",mode=\"w\") as f:\n",
    "            f.write(f\"{format_time()} | {self.__runid} \\n\")\n",
    "            f.flush()\n",
    "        self.__watcher()\n",
    "\n",
    "        #通知客户端开始\n",
    "        send_data ={\n",
    "                        \"experimentId\":self.__config[\"experiment\"],\n",
    "                        \"runName\": self.__runid,\n",
    "                        \"status\": 0\n",
    "                    }\n",
    "        resp = api(url=self.__api_notice_run,data=send_data)\n",
    "        if not resp[\"code\"] == 200:\n",
    "            raise ConnectionError\n",
    "    \n",
    "        return\n",
    "\n",
    "    def Log(self,info:dict) ->None:\n",
    "        try:\n",
    "            self.__trainning_status[\"count\"] += 1\n",
    "            self.__trainning_status[\"epoch\"].append(info)\n",
    "        except:\n",
    "            raise BaseException(\"Epoch日志采集失败\")\n",
    "        return \n",
    "    \n",
    "    def End(self) ->None:\n",
    "        \n",
    "        send_data ={\n",
    "                        \"experimentId\":self.__config[\"experiment\"],\n",
    "                        \"runName\": self.__runid,\n",
    "                        \"status\": 1\n",
    "                    }\n",
    "        resp = api(url=self.__api_notice_run,data=send_data)\n",
    "        if not resp[\"code\"] == 200:\n",
    "            raise ConnectionError\n",
    "        result_path =f\"{self.__location}/{self.__runid}/results.json\"\n",
    "        save_dict_to_json(self.__trainning_status[\"epoch\"][self.__trainning_status[\"next_start_at\"]:self.__trainning_status[\"count\"]],result_path)\n",
    "        result_csv_path=f\"{self.__location}/{self.__runid}/results.csv\"\n",
    "\n",
    "        dict = quick_analysis(status=self.__trainning_status[\"epoch\"][self.__trainning_status[\"next_start_at\"]:self.__trainning_status[\"count\"]])\n",
    "        analysis_json_path = f\"{self.__location}/{self.__runid}/analysis.json\"\n",
    "        analysis_yaml_path = f\"{self.__location}/{self.__runid}/analysis.yaml\"\n",
    "        save_dict_to_json(dict,analysis_json_path)\n",
    "        save_dict_to_yaml(dict,analysis_yaml_path)\n",
    "\n",
    "\n",
    "\n",
    "        save_list_to_csv(self.__trainning_status[\"epoch\"][self.__trainning_status[\"next_start_at\"]:self.__trainning_status[\"count\"]],result_csv_path)\n",
    "        self.__trainning_status[\"next_start_at\"] = self.__trainning_status[\"count\"]\n",
    "        last = self.__trainning_status[\"epoch\"][len(self.__trainning_status[\"epoch\"])-1]\n",
    "        last_result_path = f\"{self.__location}/{self.__runid}/last.json\"\n",
    "        save_dict_to_json(last,last_result_path)\n",
    "        self.__watcher_cpu.kill()\n",
    "        self.__watcher_gpu.kill()\n",
    "        i=0\n",
    "        while os.path.exists(f\"{self.__location}/run-{i}\"):\n",
    "            i+=1\n",
    "            self.__runid = f\"run-{i}\"\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def Submit(self) ->None:\n",
    "        try:\n",
    "            send_data = {\n",
    "                            \"experimentId\":self.__config[\"experiment\"],\n",
    "                            \"status\":1\n",
    "                        }\n",
    "\n",
    "            resp = api(url=self.__api_notice_experiment,data=send_data)\n",
    "            if not resp[\"code\"] == 200:\n",
    "                raise ConnectionError\n",
    "            with open(f\"{self.__location}/\"+\"/finish.tag\",mode=\"a\") as f:\n",
    "                f.write(f\"{format_time()} | {self.__runid} \\n\")\n",
    "                f.flush()\n",
    "            self.__save_code()\n",
    "\n",
    "            # result_csv_path = f\"{self.__location}/result.csv\"\n",
    "            # save_list_to_csv(self.__trainning_status[\"epoch\"],result_csv_path)\n",
    "\n",
    "            dict = quick_analysis(status=self.__trainning_status[\"epoch\"])\n",
    "            analysis_json_path = f\"{self.__location}/analysis.json\"\n",
    "            analysis_yaml_path = f\"{self.__location}/analysis.yaml\"\n",
    "            save_dict_to_json(dict,analysis_json_path)\n",
    "            save_dict_to_yaml(dict,analysis_yaml_path)\n",
    "        except:\n",
    "            raise BaseException(\"END ERROR\")\n",
    "        return\n",
    "    \n",
    "    def __save_config(self) ->None:\n",
    "        try:\n",
    "            config_path_json = self.__location +\"/\"+\"config.json\"\n",
    "            save_dict_to_json(self.__config,config_path_json)\n",
    "            config_path_yaml = self.__location +\"/\"+\"config.yaml\"\n",
    "            save_dict_to_yaml(self.__config,config_path_yaml)\n",
    "        except:\n",
    "            raise BaseException(\"保存配置信息失败 \\n\")\n",
    "        return\n",
    "\n",
    "    def __save_code(self,path=[\"datasets\"])->None:\n",
    "        ignore_path = [*path]\n",
    "        try:\n",
    "            if os.path.exists(\".path_ignore\"):\n",
    "                with open(\".path_ignore\") as f:\n",
    "                    line =  f.readline()\n",
    "                    while line:\n",
    "                        ignore_path.append(line.strip())\n",
    "                        line = f.readline()\n",
    "            shutil.copytree(src=self.__codedir,dst=self.__savedir,dirs_exist_ok=True,ignore=shutil.ignore_patterns(*ignore_path))\n",
    "        except:\n",
    "            raise BaseException(\"备份代码失败 \\n\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = NewLogger(\n",
    "\n",
    "#     conf={\n",
    "#     'access_token':\"access_token\",\n",
    "#     'project':\"project\", \n",
    "#     \"description\":\"description\"},\n",
    "\n",
    "#     info={\n",
    "#     \"learnning_rate\":0.2,\n",
    "#     \"epoch\":10,\n",
    "#     \"batch_size\":8\n",
    "#     }\n",
    "# )\n",
    "# i = 0\n",
    "# while i <5 :\n",
    "#         log.Run()\n",
    "#         j = 0\n",
    "#         while j<5:\n",
    "#             log.Log({\"acc\":0.8})\n",
    "#             log.Save([\"./1.png\"])\n",
    "#             j+=1\n",
    "#         log.End()\n",
    "#         i+=1\n",
    "\n",
    "# log.End()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代理客户端验证通过 \n",
      "\n",
      "采集系统信息\n",
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN,self).__init__()\n",
    "            '''\n",
    "            一般来说，卷积网络包括以下内容：\n",
    "            1.卷积层\n",
    "            2.神经网络\n",
    "            3.池化层\n",
    "            '''\n",
    "            self.conv1=nn.Sequential(\n",
    "                nn.Conv2d(              #--> (1,28,28)\n",
    "                    in_channels=1,      #传入的图片灰度图\n",
    "                    out_channels=16,    #输出的图片\n",
    "                    kernel_size=5,      #卷积核为5*5\n",
    "                    stride=1,           #跳步步长\n",
    "                    padding=2,          #边框补全，其计算公式=（kernel_size-1）/2=(5-1)/2=2\n",
    "                ),    # 2d代表二维卷积           --> (16,28,28)\n",
    "                nn.ReLU(),              #非线性激活层\n",
    "                nn.MaxPool2d(kernel_size=2),    #设定这里的扫描区域为2*2，且取出该2*2中的最大值          --> (16,14,14)\n",
    "            )\n",
    "\n",
    "            self.conv2=nn.Sequential(\n",
    "                nn.Conv2d(              #       --> (16,14,14)\n",
    "                    in_channels=16,     #这里的输入是上层的输出为16层\n",
    "                    out_channels=32,    #在这里我们需要将其输出为32层\n",
    "                    kernel_size=5,      #代表扫描的区域点为5*5\n",
    "                    stride=1,           #就是每隔多少步跳一下\n",
    "                    padding=2,          #边框补全，其计算公式=（kernel_size-1）/2=(5-1)/2=\n",
    "                ),                      #   --> (32,14,14)\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),    #设定这里的扫描区域为2*2，且取出该2*2中的最大值     --> (32,7,7)，这里是三维数据\n",
    "            )\n",
    "\n",
    "            self.out=nn.Linear(32*7*7,10)       #注意一下这里的数据是二维的数据\n",
    "\n",
    "        def forward(self,x):\n",
    "            x=self.conv1(x)\n",
    "            x=self.conv2(x)     #（batch,32,7,7）\n",
    "            #然后接下来进行一下扩展展平的操作，将三维数据转为二维的数据\n",
    "            x=x.view(x.size(0),-1)    #(batch ,32 * 7 * 7)\n",
    "            output=self.out(x)\n",
    "            return output\n",
    "\n",
    "#Hyper prameters\n",
    "EPOCH=3\n",
    "BATCH_SIZE=64\n",
    "LR=0.001\n",
    "DOWNLOAD_MNIST=False\n",
    "\n",
    "log = NewLogger(\n",
    "\n",
    "    conf={\n",
    "    'access_token':\"access_token\",\n",
    "    'project':\"project\", \n",
    "    \"description\":\"description\"},\n",
    "\n",
    "    info={\n",
    "    \"learnning_rate\":LR,\n",
    "    \"epoch\":EPOCH,\n",
    "    \"batch_size\":BATCH_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "run_count = 0\n",
    "while run_count<3:\n",
    "\n",
    "    log.Run()\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\") \n",
    "        print(\"Running on the GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Running on the CPU\")\n",
    "\n",
    "\n",
    "\n",
    "    train_data=torchvision.datasets.MNIST(\n",
    "        root='./mnist',\n",
    "        train=True,\n",
    "        transform=torchvision.transforms.ToTensor(),    #将下载的文件转换成pytorch认识的tensor类型，且将图片的数值大小从（0-255）归一化到（0-1）\n",
    "        download=DOWNLOAD_MNIST\n",
    "    )\n",
    "\n",
    "    # print(train_data.data.size())\n",
    "    # print(train_data.targets.size())\n",
    "    # plt.imshow(train_data.data[0].numpy(),cmap='gray')\n",
    "    # plt.title('%i'%train_data.targets[0])\n",
    "    # plt.show()\n",
    "\n",
    "    train_loader=Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_data=torchvision.datasets.MNIST(\n",
    "        root='./mnist',\n",
    "        train=False,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        test_x=Variable(torch.unsqueeze(test_data.data, dim=1)).type(torch.cuda.FloatTensor)[:2000]/255  \n",
    "        test_y=test_data.targets[:2000]\n",
    "        test_y.cuda()\n",
    "\n",
    "\n",
    "            \n",
    "    cnn=CNN()\n",
    "    # print(cnn)\n",
    "    cnn.to(device=device)\n",
    "\n",
    "    # 添加优化方法\n",
    "    optimizer=torch.optim.Adam(cnn.parameters(),lr=LR)\n",
    "    # 指定损失函数使用交叉信息熵\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "    step=0\n",
    "    for e in range(EPOCH):\n",
    "        #加载训练数据\n",
    "        for step,(x,y) in enumerate(train_loader):\n",
    "            #分别得到训练数据的x和y的取值\n",
    "            b_x=Variable(x.to(device))\n",
    "            b_y=Variable(y.to(device))\n",
    "            output=cnn(b_x)         #调用模型预测\n",
    "            loss=loss_fn(output,b_y)#计算损失值\n",
    "            optimizer.zero_grad()   #每一次循环之前，将梯度清零\n",
    "            loss.backward()         #反向传播\n",
    "            optimizer.step()        #梯度下降\n",
    "\n",
    "            count = 1\n",
    "            #每执行count次，输出一下当前epoch、loss、accuracy\n",
    "            if (step%count==0):\n",
    "                #计算一下模型预测正确率\n",
    "                test_output=cnn(test_x)\n",
    "                y_pred=torch.max(test_output,1)[1].data.squeeze()\n",
    "                accuracy=sum(y_pred==test_y.cuda()).item()/test_y.cuda().size(0)\n",
    "                log.Log({\"epoch\":e,\"loss\":loss.item(),\"accuracy\":accuracy})\n",
    "                log.Save([\"1.png\"])\n",
    "                # print('now epoch :  ', epoch, '   |  loss : %.4f ' % loss.item(), '     |   accuracy :   ' , accuracy)\n",
    "\n",
    "    '''\n",
    "    打印十个测试集的结果\n",
    "    '''\n",
    "\n",
    "    test_output=cnn(test_x[:10])\n",
    "    y_pred=torch.max(test_output,1)[1].data.squeeze()       #选取最大可能的数值所在的位置\n",
    "    print(y_pred.tolist(),'predecton Result')\n",
    "    print(test_y[:10].tolist(),'Real Result')\n",
    "    log.End()\n",
    "    run_count +=1\n",
    "\n",
    "log.Submit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"model.pt\"\n",
    "\n",
    "# torch.save(cnn , model_path)\n",
    "# log.SaveFile([model_path])\n",
    "# log.End()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "# y_data = torch.Tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "# class LinearModel(torch.nn.Module): \n",
    "#     def __init__(self):\n",
    "#         super(LinearModel, self).__init__() \n",
    "#         self.linear = torch.nn.Linear(1, 1)\n",
    "#     def forward(self, x): \n",
    "#         y_pred = self.linear(x) \n",
    "#         return y_pred\n",
    "\n",
    "# log = NewLogger(\n",
    "#         # 创建实例\n",
    "#     # 1.实例创建时请求本地客户端 （等客户端完成）\n",
    "#     # 确认客户端启动后（三次），继续执行； （等客户端完成）\n",
    "#     # 否则抛出错误，程序终止。 （等客户端完成）\n",
    "#     conf={\n",
    "#     'access_token':\"access_token\",\n",
    "#     'project':\"project\", \n",
    "#     \"discription\":\"discription\"},\n",
    "#     # 开始任务\n",
    "#     # 1.记录超参（常量）（ok）\n",
    "#     # 2.记录运行环境硬件信息(一次)（ok）\n",
    "#     # 3.通知客户端，实验开始 （等客户端完成）\n",
    "#     # 4.开启cpu、内存记录信息进程 （ok）\n",
    "#     # 5.开启显卡核心、显存信息记录进程 (ok)\n",
    "#     # 6.记录conda信息（ok）\n",
    "#     # 7.输入用户token、工程id、用户描述, 生成实验名称，按条件生成config.json\n",
    "#     # 8.记录requirement.txt信息(待写)\n",
    "#     # log.Start(info={\"learnning_rate\":0.002,\"epoch\":10}) \n",
    "#     info={\n",
    "#     \"learnning_rate\":0.002,\n",
    "#     \"epoch\":30,\n",
    "#     \"batch_size\":8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# model = LinearModel()\n",
    "# criterion = torch.nn.MSELoss(size_average=False)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# for epoch in range(30):\n",
    "#     log.EpochStart()\n",
    "#     y_pred = model(x_data)\n",
    "#     loss = criterion(y_pred, y_data) \n",
    "#     print(epoch, loss.item())\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     log.EpochLog({\"loss\":loss.item()})\n",
    "#     optimizer.step()\n",
    "#     log.SaveFile([\"./main.py\"])\n",
    "#     log.EpochEnd()\n",
    "# print('w = ', model.linear.weight.item())\n",
    "# print('b = ', model.linear.bias.item())\n",
    "# x_test = torch.Tensor([[4.0]]) \n",
    "# y_test = model(x_test) \n",
    "# print('y_pred = ', y_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = NewLogger(\n",
    "#     # 创建实例\n",
    "#     # 1.实例创建时请求本地客户端 （等客户端完成）\n",
    "#     # 确认客户端启动后（三次），继续执行； （等客户端完成）\n",
    "#     # 否则抛出错误，程序终止。 （等客户端完成）\n",
    "#     conf={\n",
    "#     'access_token':\"access_token\",\n",
    "#     'project':\"project\", \n",
    "#     \"discription\":\"discription\"},\n",
    "#     # 开始任务\n",
    "#     # 1.记录超参（常量）（ok）\n",
    "#     # 2.记录运行环境硬件信息(一次)（ok）\n",
    "#     # 3.通知客户端，实验开始 （等客户端完成）\n",
    "#     # 4.开启cpu、内存记录信息进程 （ok）\n",
    "#     # 5.开启显卡核心、显存信息记录进程 (ok)\n",
    "#     # 6.记录conda信息（ok）\n",
    "#     # 7.输入用户token、工程id、用户描述, 生成实验名称，按条件生成config.json\n",
    "#     # 8.记录requirement.txt信息(待写)\n",
    "#     # log.Start(info={\"learnning_rate\":0.002,\"epoch\":10}) \n",
    "#     info={\n",
    "#     \"learnning_rate\":0.002,\n",
    "#     \"epoch\":30,\n",
    "#     \"batch_size\":8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# i=0\n",
    "# # 模拟epoch开始循环\n",
    "# while i<30 :\n",
    "\n",
    "#     # 1. 通知客户端一个批处理开始 （等客户端完成）\n",
    "#     # 2. 记录批处理开始时间\n",
    "#     log.EpochStart()\n",
    "\n",
    "#     # 模拟任务代码\n",
    "#     time.sleep(1)\n",
    "    \n",
    "#     # 循环结束\n",
    "#     # 1. 为运行id文件夹添加finish.tag (ok)\n",
    "#     # 2. 通知客户端，一个批处理结束 （等客户端完成）\n",
    "#     # 3. 记录回调中要记录的参数 (ok) \n",
    "#     # 4. json转csv(ok)\n",
    "\n",
    "#     j=0\n",
    "#     #批处理循环\n",
    "#     while j<1000:\n",
    "#         log.EpochLog({\"acc\":0.83,\"loss\":0.02,\"sth1\":0.32,\"sth2\":0.43})\n",
    "#         j+=1\n",
    "#         continue\n",
    "\n",
    "#     #保存文件至指定文件夹方法（ok）\n",
    "#     log.SaveFile([\"./main.py\"])\n",
    "#     i+=1\n",
    "#     log.EpochEnd()\n",
    "\n",
    "# # 结束任务\n",
    "# # 1.记录运行代码 (ok),忽略“datasets目录”\n",
    "# # 2.通知客户端结束实验（等客户端完成）\n",
    "# # 3. finish.tag(ok)\n",
    "# # 4. 最大值，最小值，均值，方差，标准差（ok）\n",
    "# log.End()\n",
    "\n",
    "# # 工具方法：打印日志类状态\n",
    "# log.ShowStatus()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
